{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(print \"downloading training data...\")? (<ipython-input-2-70bb70ded1c4>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-70bb70ded1c4>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    print \"downloading training data...\"\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(print \"downloading training data...\")?\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# to run:\n",
    "# python wide_and_deep_keras.py --method method --model_type model_type\n",
    "# --train_data train_path --test_data test_path\n",
    "# Examples:\n",
    "# 1_. wide and deep model for logistic regression (defaults)\n",
    "# python wide_and_deep_keras.py\n",
    "# 2_. deep model for multiclass classification\n",
    "# python wide_and_deep_keras.py --method multiclass --model_type deep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, concatenate, Embedding, Reshape\n",
    "from keras.layers import Merge, Flatten, merge, Lambda, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2, l1_l2\n",
    "\n",
    "\n",
    "def maybe_download(train_data,test_data):\n",
    "    \"\"\"if adult data \"train.csv\" and \"test.csv\" are not in your directory,\n",
    "    download them.\n",
    "    \"\"\"\n",
    "\n",
    "    COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "               \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "               \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "               \"income_bracket\"]\n",
    "\n",
    "    if not os.path.exists(train_data):\n",
    "        print (\"downloading training data...\")\n",
    "        df_train = pd.read_csv(\"http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "            names=COLUMNS, skipinitialspace=True)\n",
    "    else:\n",
    "        df_train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "    if not os.path.exists(test_data):\n",
    "        print (\"downloading testing data...\")\n",
    "        df_test = pd.read_csv(\"http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "            names=COLUMNS, skipinitialspace=True, skiprows=1)\n",
    "    else:\n",
    "        df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def cross_columns(x_cols):\n",
    "    \"\"\"simple helper to build the crossed columns in a pandas dataframe\n",
    "    \"\"\"\n",
    "    crossed_columns = dict()\n",
    "    colnames = ['_'.join(x_c) for x_c in x_cols]\n",
    "    for cname, x_c in zip(colnames, x_cols):\n",
    "        crossed_columns[cname] = x_c\n",
    "    return crossed_columns\n",
    "\n",
    "\n",
    "def val2idx(df, cols):\n",
    "    \"\"\"helper to index categorical columns before embeddings.\n",
    "    \"\"\"\n",
    "    val_types = dict()\n",
    "    for c in cols:\n",
    "        val_types[c] = df[c].unique()\n",
    "\n",
    "    val_to_idx = dict()\n",
    "    for k, v in val_types.iteritems():\n",
    "        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
    "\n",
    "    for k, v in val_to_idx.iteritems():\n",
    "        df[k] = df[k].apply(lambda x: v[x])\n",
    "\n",
    "    unique_vals = dict()\n",
    "    for c in cols:\n",
    "        unique_vals[c] = df[c].nunique()\n",
    "\n",
    "    return df, unique_vals\n",
    "\n",
    "\n",
    "def onehot(x):\n",
    "    return np.array(OneHotEncoder().fit_transform(x).todense())\n",
    "\n",
    "\n",
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, embeddings_regularizer=l2(reg))(inp)\n",
    "\n",
    "\n",
    "def continous_input(name):\n",
    "    inp = Input(shape=(1,), dtype='float32', name=name)\n",
    "    return inp, Reshape((1, 1))(inp)\n",
    "\n",
    "\n",
    "def wide(df_train, df_test, wide_cols, x_cols, target, model_type, method):\n",
    "    \"\"\"Run the wide (linear) model.\n",
    "    Params:\n",
    "    -------\n",
    "    df_train, df_test: train and test datasets\n",
    "    wide_cols   : columns to be used to fit the wide model\n",
    "    x_cols      : columns to be \"crossed\"\n",
    "    target      : the target feature\n",
    "    model_type  : accepts \"wide\" and \"wide_deep\" (or anything that is not\n",
    "    \"wide\"). If \"wide_deep\" the function will build and return the inputs\n",
    "    but NOT run any model.\n",
    "    method      : the fitting method. accepts regression, logistic and multiclass\n",
    "    Returns:\n",
    "    --------\n",
    "    if \"wide\":\n",
    "    print the results obtained on the test set in the terminal.\n",
    "    if \"wide_deep\":\n",
    "    X_train, y_train, X_test, y_test: the inputs required to build wide and deep\n",
    "    \"\"\"\n",
    "\n",
    "    df_train['IS_TRAIN'] = 1\n",
    "    df_test['IS_TRAIN'] = 0\n",
    "    df_wide = pd.concat([df_train, df_test])\n",
    "\n",
    "    # my understanding on how to replicate what layers.crossed_column does. One\n",
    "    # can read here: https://www.tensorflow.org/tutorials/linear.\n",
    "    crossed_columns_d = cross_columns(x_cols)\n",
    "    categorical_columns = list(\n",
    "        df_wide.select_dtypes(include=['object']).columns)\n",
    "\n",
    "    wide_cols += crossed_columns_d.keys()\n",
    "\n",
    "    for k, v in crossed_columns_d.iteritems():\n",
    "        df_wide[k] = df_wide[v].apply(lambda x: '-'.join(x), axis=1)\n",
    "\n",
    "    df_wide = df_wide[wide_cols + [target] + ['IS_TRAIN']]\n",
    "\n",
    "    dummy_cols = [\n",
    "        c for c in wide_cols if c in categorical_columns + crossed_columns_d.keys()]\n",
    "    df_wide = pd.get_dummies(df_wide, columns=[x for x in dummy_cols])\n",
    "\n",
    "    train = df_wide[df_wide.IS_TRAIN == 1].drop('IS_TRAIN', axis=1)\n",
    "    test = df_wide[df_wide.IS_TRAIN == 0].drop('IS_TRAIN', axis=1)\n",
    "\n",
    "    # make sure all columns are in the same order and life is easier\n",
    "    cols = [target] + [c for c in train.columns if c != target]\n",
    "    train = train[cols]\n",
    "    test = test[cols]\n",
    "\n",
    "    X_train = train.values[:, 1:]\n",
    "    y_train = train.values[:, 0].reshape(-1, 1)\n",
    "    X_test = test.values[:, 1:]\n",
    "    y_test = test.values[:, 0].reshape(-1, 1)\n",
    "    if method == 'multiclass':\n",
    "        y_train = onehot(y_train)\n",
    "        y_test = onehot(y_test)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test  = scaler.fit_transform(X_test)\n",
    "\n",
    "    if model_type == 'wide':\n",
    "\n",
    "        activation, loss, metrics = fit_param[method]\n",
    "        # metrics parameter needs to be passed as a list or dict\n",
    "        if metrics:\n",
    "            metrics = [metrics]\n",
    "\n",
    "        # simply connecting the features to an output layer\n",
    "        wide_inp = Input(shape=(X_train.shape[1],), dtype='float32', name='wide_inp')\n",
    "        w = Dense(y_train.shape[1], activation=activation)(wide_inp)\n",
    "        wide = Model(wide_inp, w)\n",
    "        wide.compile(Adam(0.01), loss=loss, metrics=metrics)\n",
    "        wide.fit(X_train, y_train, nb_epoch=10, batch_size=64)\n",
    "        results = wide.evaluate(X_test, y_test)\n",
    "\n",
    "        print \"\\n\", results\n",
    "\n",
    "    else:\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def deep(df_train, df_test, embedding_cols, cont_cols, target, model_type, method):\n",
    "    \"\"\"Run the deep model. Two layers of 100 and 50 neurons. In a decent,\n",
    "    finished code these would be tunable.\n",
    "    Params:\n",
    "    -------\n",
    "    df_train, df_test: train and test datasets\n",
    "    embedding_cols: columns to be passed as embeddings\n",
    "    cont_cols     : numerical columns to be combined with the embeddings\n",
    "    target        : the target feature\n",
    "    model_type    : accepts \"deep\" and \"wide_deep\" (or anything that is not\n",
    "    \"wide\"). If \"wide_deep\" the function will build and returns the inputs\n",
    "    but NOT run any model\n",
    "    method        : the fitting method. accepts regression, logistic and multiclass\n",
    "    Returns:\n",
    "    --------\n",
    "    if \"deep\":\n",
    "    print the results obtained on the test set in the terminal.\n",
    "    if \"wide_deep\":\n",
    "    X_train, y_train, X_test, y_test: the inputs required to build wide and deep\n",
    "    inp_embed, inp_layer: the embedding layers and the input tensors for Model()\n",
    "    \"\"\"\n",
    "\n",
    "    df_train['IS_TRAIN'] = 1\n",
    "    df_test['IS_TRAIN'] = 0\n",
    "    df_deep = pd.concat([df_train, df_test])\n",
    "\n",
    "    deep_cols = embedding_cols + cont_cols\n",
    "\n",
    "    # I 'd say that adding numerical columns to embeddings can be done in two ways:\n",
    "    # 1_. normalise the values in the dataframe and pass them to the network\n",
    "    # 2_. add BatchNormalization() layer. (I am not entirely sure this is right)\n",
    "    # I'd say option 1 is the correct one. 2 performs better, which does not say much, but...\n",
    "\n",
    "    # 1_. Scaling in the dataframe\n",
    "    # scaler = MinMaxScaler()\n",
    "    # cont_df = df_deep[cont_cols]\n",
    "    # cont_norm_df = pd.DataFrame(scaler.fit_transform(df_train[cont_cols]))\n",
    "    # cont_norm_df.columns = cont_cols\n",
    "    # for c in cont_cols: df_deep[c] = cont_norm_df[c]\n",
    "\n",
    "    df_deep, unique_vals = val2idx(df_deep, embedding_cols)\n",
    "\n",
    "    train = df_deep[df_deep.IS_TRAIN == 1].drop('IS_TRAIN', axis=1)\n",
    "    test = df_deep[df_deep.IS_TRAIN == 0].drop('IS_TRAIN', axis=1)\n",
    "\n",
    "    embeddings_tensors = []\n",
    "    n_factors = 8\n",
    "    reg = 1e-3\n",
    "    for ec in embedding_cols:\n",
    "        layer_name = ec + '_inp'\n",
    "        t_inp, t_build = embedding_input(\n",
    "            layer_name, unique_vals[ec], n_factors, reg)\n",
    "        embeddings_tensors.append((t_inp, t_build))\n",
    "        del(t_inp, t_build)\n",
    "\n",
    "    continuous_tensors = []\n",
    "    for cc in cont_cols:\n",
    "        layer_name = cc + '_in'\n",
    "        t_inp, t_build = continous_input(layer_name)\n",
    "        continuous_tensors.append((t_inp, t_build))\n",
    "        del(t_inp, t_build)\n",
    "\n",
    "    X_train = [train[c] for c in deep_cols]\n",
    "    y_train = np.array(train[target].values).reshape(-1, 1)\n",
    "    X_test = [test[c] for c in deep_cols]\n",
    "    y_test = np.array(test[target].values).reshape(-1, 1)\n",
    "\n",
    "    if method == 'multiclass':\n",
    "        y_train = onehot(y_train)\n",
    "        y_test = onehot(y_test)\n",
    "\n",
    "    inp_layer =  [et[0] for et in embeddings_tensors]\n",
    "    inp_layer += [ct[0] for ct in continuous_tensors]\n",
    "    inp_embed =  [et[1] for et in embeddings_tensors]\n",
    "    inp_embed += [ct[1] for ct in continuous_tensors]\n",
    "\n",
    "    if model_type == 'deep':\n",
    "\n",
    "        activation, loss, metrics = fit_param[method]\n",
    "        if metrics:\n",
    "            metrics = [metrics]\n",
    "\n",
    "        d = merge(inp_embed, mode='concat')\n",
    "        d = Flatten()(d)\n",
    "        # 2_. layer to normalise continous columns with the embeddings\n",
    "        d = BatchNormalization()(d)\n",
    "        d = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(d)\n",
    "        # d = Dropout(0.5)(d) # Dropout don't seem to help in this model\n",
    "        d = Dense(50, activation='relu')(d)\n",
    "        # d = Dropout(0.5)(d) # Dropout don't seem to help in this model\n",
    "        d = Dense(y_train.shape[1], activation=activation)(d)\n",
    "        deep = Model(inp_layer, d)\n",
    "        deep.compile(Adam(0.01), loss=loss, metrics=metrics)\n",
    "        deep.fit(X_train, y_train, batch_size=64, nb_epoch=10)\n",
    "        results = deep.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "        print \"\\n\", results\n",
    "\n",
    "    else:\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, inp_embed, inp_layer\n",
    "\n",
    "\n",
    "def wide_deep(df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols, method):\n",
    "    \"\"\"Run the wide and deep model. Parameters are the same as those for the\n",
    "    wide and deep functions\n",
    "    \"\"\"\n",
    "\n",
    "    # Default model_type is \"wide_deep\"\n",
    "    X_train_wide, y_train_wide, X_test_wide, y_test_wide = \\\n",
    "        wide(df_train, df_test, wide_cols, x_cols, target, model_type, method)\n",
    "\n",
    "    X_train_deep, y_train_deep, X_test_deep, y_test_deep, deep_inp_embed, deep_inp_layer = \\\n",
    "        deep(df_train, df_test, embedding_cols,cont_cols, target, model_type, method)\n",
    "\n",
    "    X_tr_wd = [X_train_wide] + X_train_deep\n",
    "    Y_tr_wd = y_train_deep  # wide or deep is the same here\n",
    "    X_te_wd = [X_test_wide] + X_test_deep\n",
    "    Y_te_wd = y_test_deep  # wide or deep is the same here\n",
    "\n",
    "    activation, loss, metrics = fit_param[method]\n",
    "    if metrics: metrics = [metrics]\n",
    "\n",
    "    # WIDE\n",
    "    w = Input(shape=(X_train_wide.shape[1],), dtype='float32', name='wide')\n",
    "\n",
    "    # DEEP: the output of the 50 neurons layer will be the deep-side input\n",
    "    d = merge(deep_inp_embed, mode='concat')\n",
    "    d = Flatten()(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(d)\n",
    "    d = Dense(50, activation='relu', name='deep')(d)\n",
    "\n",
    "    # WIDE + DEEP\n",
    "    wd_inp = concatenate([w, d])\n",
    "    wd_out = Dense(Y_tr_wd.shape[1], activation=activation, name='wide_deep')(wd_inp)\n",
    "    wide_deep = Model(inputs=[w] + deep_inp_layer, outputs=wd_out)\n",
    "    wide_deep.compile(optimizer=Adam(lr=0.01), loss=loss, metrics=metrics)\n",
    "    wide_deep.fit(X_tr_wd, Y_tr_wd, nb_epoch=10, batch_size=128)\n",
    "\n",
    "    # Maybe you want to schedule a second search with lower learning rate\n",
    "    # wide_deep.optimizer.lr = 0.0001\n",
    "    # wide_deep.fit(X_tr_wd, Y_tr_wd, nb_epoch=10, batch_size=128)\n",
    "\n",
    "    results = wide_deep.evaluate(X_te_wd, Y_te_wd)\n",
    "\n",
    "    print \"\\n\", results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--method\", type=str, default=\"logistic\",help=\"fitting method\")\n",
    "    ap.add_argument(\"--model_type\", type=str, default=\"wide_deep\",help=\"wide, deep or both\")\n",
    "    ap.add_argument(\"--train_data\", type=str, default=\"train.csv\")\n",
    "    ap.add_argument(\"--test_data\", type=str, default=\"test.csv\")\n",
    "    args = vars(ap.parse_args())\n",
    "    method = args[\"method\"]\n",
    "    model_type = args['model_type']\n",
    "    train_data = args['train_data']\n",
    "    test_data = args['test_data']\n",
    "\n",
    "    fit_param = dict()\n",
    "    fit_param['logistic']   = ('sigmoid', 'binary_crossentropy', 'accuracy')\n",
    "    fit_param['regression'] = (None, 'mse', None)\n",
    "    fit_param['multiclass'] = ('softmax', 'categorical_crossentropy', 'accuracy')\n",
    "\n",
    "    df_train, df_test = maybe_download(train_data, test_data)\n",
    "\n",
    "    # Add a feature to illustrate the logistic regression example\n",
    "    df_train['income_label'] = (\n",
    "        df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "    df_test['income_label'] = (\n",
    "        df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "    # Add a feature to illustrate multiclass classification\n",
    "    age_groups = [0, 25, 65, 90]\n",
    "    age_labels = range(len(age_groups) - 1)\n",
    "    df_train['age_group'] = pd.cut(\n",
    "        df_train['age'], age_groups, labels=age_labels)\n",
    "    df_test['age_group'] = pd.cut(\n",
    "        df_test['age'], age_groups, labels=age_labels)\n",
    "\n",
    "    # columns for wide model\n",
    "    wide_cols = ['age','hours_per_week','education', 'relationship', 'workclass',\n",
    "                 'occupation','native_country','gender']\n",
    "    x_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "\n",
    "    # columns for deep model\n",
    "    embedding_cols = ['education', 'relationship', 'workclass', 'occupation',\n",
    "                      'native_country']\n",
    "    cont_cols = [\"age\",\"hours_per_week\"]\n",
    "\n",
    "    # target for logistic\n",
    "    target = 'income_label'\n",
    "\n",
    "    # # A set-up for multiclass classification would be:\n",
    "    # # change method to multiclass\n",
    "    # wide_cols = [\"gender\", \"native_country\", \"education\", \"occupation\", \"workclass\",\n",
    "    #              \"relationship\"]\n",
    "    # x_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "\n",
    "    # # columns for deep model\n",
    "    # embedding_cols = ['education', 'relationship', 'workclass', 'occupation',\n",
    "    #                   'native_country']\n",
    "    # cont_cols = [\"hours_per_week\"]\n",
    "\n",
    "    # # target\n",
    "    # target = 'age_group'\n",
    "\n",
    "    if model_type == 'wide':\n",
    "        wide(df_train, df_test, wide_cols, x_cols, target, model_type, method)\n",
    "    elif model_type == 'deep':\n",
    "        deep(df_train, df_test, embedding_cols, cont_cols, target, model_type, method)\n",
    "    else:\n",
    "        wide_deep(df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
