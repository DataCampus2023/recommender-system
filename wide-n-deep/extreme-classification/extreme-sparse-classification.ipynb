{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import extreme multi-label dataset\n",
    "- in extreme-sparse dataset, test wide n deep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/bibtex\n"
     ]
    }
   ],
   "source": [
    "dataset=\"bibtex\"\n",
    "data_dir='data/{}'.format(dataset)\n",
    "print(data_dir)\n",
    "def load_input():\n",
    "    data = list(loadmat(data_dir + '/input.mat')['data'][0][0])\n",
    "    return data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4880, 1836)\n",
      "(4880, 159)\n"
     ]
    }
   ],
   "source": [
    "train_X = train_X.toarray()\n",
    "train_Y = train_Y.toarray()\n",
    "test_X = test_X.toarray()\n",
    "test_Y = test_Y.toarray()\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1826</th>\n",
       "      <th>1827</th>\n",
       "      <th>1828</th>\n",
       "      <th>1829</th>\n",
       "      <th>1830</th>\n",
       "      <th>1831</th>\n",
       "      <th>1832</th>\n",
       "      <th>1833</th>\n",
       "      <th>1834</th>\n",
       "      <th>1835</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1836 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   1826  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    1.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "\n",
       "   1827  1828  1829  1830  1831  1832  1833  1834  1835  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 1836 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_X).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding wide-n-deep with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3904 samples, validate on 976 samples\n",
      "Epoch 1/30\n",
      "3904/3904 [==============================] - 1s 164us/step - loss: 11.8599 - acc: 0.0236 - val_loss: 11.8551 - val_acc: 0.0625\n",
      "Epoch 2/30\n",
      "3904/3904 [==============================] - 0s 117us/step - loss: 10.9283 - acc: 0.0774 - val_loss: 11.3183 - val_acc: 0.0840\n",
      "Epoch 3/30\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 10.2314 - acc: 0.1283 - val_loss: 10.8678 - val_acc: 0.1650\n",
      "Epoch 4/30\n",
      "3904/3904 [==============================] - 0s 114us/step - loss: 9.5705 - acc: 0.2059 - val_loss: 10.3990 - val_acc: 0.1834\n",
      "Epoch 5/30\n",
      "3904/3904 [==============================] - 0s 116us/step - loss: 8.9016 - acc: 0.2403 - val_loss: 9.9461 - val_acc: 0.1906\n",
      "Epoch 6/30\n",
      "3904/3904 [==============================] - 0s 114us/step - loss: 8.2707 - acc: 0.2713 - val_loss: 9.5387 - val_acc: 0.2039\n",
      "Epoch 7/30\n",
      "3904/3904 [==============================] - 0s 116us/step - loss: 7.6986 - acc: 0.3186 - val_loss: 9.1815 - val_acc: 0.2059\n",
      "Epoch 8/30\n",
      "3904/3904 [==============================] - 0s 118us/step - loss: 7.1868 - acc: 0.3609 - val_loss: 8.8949 - val_acc: 0.2316\n",
      "Epoch 9/30\n",
      "3904/3904 [==============================] - 0s 116us/step - loss: 6.7465 - acc: 0.3899 - val_loss: 8.6616 - val_acc: 0.2285\n",
      "Epoch 10/30\n",
      "3904/3904 [==============================] - 0s 115us/step - loss: 6.3594 - acc: 0.4073 - val_loss: 8.4619 - val_acc: 0.2449\n",
      "Epoch 11/30\n",
      "3904/3904 [==============================] - 0s 115us/step - loss: 6.0296 - acc: 0.4393 - val_loss: 8.3482 - val_acc: 0.2531\n",
      "Epoch 12/30\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 5.7425 - acc: 0.4524 - val_loss: 8.2247 - val_acc: 0.2592\n",
      "Epoch 13/30\n",
      "3904/3904 [==============================] - 0s 114us/step - loss: 5.4954 - acc: 0.4641 - val_loss: 8.1430 - val_acc: 0.2715\n",
      "Epoch 14/30\n",
      "3904/3904 [==============================] - 0s 112us/step - loss: 5.2686 - acc: 0.4772 - val_loss: 8.0883 - val_acc: 0.2766\n",
      "Epoch 15/30\n",
      "3904/3904 [==============================] - 0s 127us/step - loss: 5.0730 - acc: 0.4844 - val_loss: 8.0271 - val_acc: 0.2900\n",
      "Epoch 16/30\n",
      "3904/3904 [==============================] - 0s 116us/step - loss: 4.8969 - acc: 0.4977 - val_loss: 8.0093 - val_acc: 0.2859\n",
      "Epoch 17/30\n",
      "3904/3904 [==============================] - 0s 113us/step - loss: 4.7399 - acc: 0.5000 - val_loss: 7.9896 - val_acc: 0.2869\n",
      "Epoch 18/30\n",
      "3904/3904 [==============================] - 0s 122us/step - loss: 4.5945 - acc: 0.5133 - val_loss: 7.9634 - val_acc: 0.2900\n",
      "Epoch 19/30\n",
      "3904/3904 [==============================] - 0s 125us/step - loss: 4.4697 - acc: 0.5205 - val_loss: 7.9601 - val_acc: 0.2838\n",
      "Epoch 20/30\n",
      "3904/3904 [==============================] - 0s 113us/step - loss: 4.3449 - acc: 0.5236 - val_loss: 7.9479 - val_acc: 0.2889\n",
      "Epoch 21/30\n",
      "3904/3904 [==============================] - 0s 116us/step - loss: 4.2349 - acc: 0.5310 - val_loss: 7.9579 - val_acc: 0.2961\n",
      "Epoch 22/30\n",
      "3904/3904 [==============================] - 0s 115us/step - loss: 4.1370 - acc: 0.5336 - val_loss: 7.9704 - val_acc: 0.2889\n",
      "Epoch 23/30\n",
      "3904/3904 [==============================] - 0s 115us/step - loss: 4.0465 - acc: 0.5364 - val_loss: 8.0040 - val_acc: 0.2910\n",
      "Epoch 24/30\n",
      "3904/3904 [==============================] - 0s 124us/step - loss: 3.9663 - acc: 0.5351 - val_loss: 8.0317 - val_acc: 0.2889\n",
      "Epoch 25/30\n",
      "3904/3904 [==============================] - 0s 123us/step - loss: 3.8880 - acc: 0.5453 - val_loss: 8.0326 - val_acc: 0.2982\n",
      "Epoch 26/30\n",
      "3904/3904 [==============================] - 0s 114us/step - loss: 3.8152 - acc: 0.5453 - val_loss: 8.0680 - val_acc: 0.2889\n",
      "Epoch 27/30\n",
      "3904/3904 [==============================] - 0s 128us/step - loss: 3.7509 - acc: 0.5492 - val_loss: 8.1280 - val_acc: 0.2879\n",
      "Epoch 28/30\n",
      "3904/3904 [==============================] - 0s 115us/step - loss: 3.6904 - acc: 0.5551 - val_loss: 8.1661 - val_acc: 0.2951\n",
      "Epoch 29/30\n",
      "3904/3904 [==============================] - 1s 129us/step - loss: 3.6328 - acc: 0.5533 - val_loss: 8.1729 - val_acc: 0.2910\n",
      "Epoch 30/30\n",
      "3904/3904 [==============================] - 0s 114us/step - loss: 3.5828 - acc: 0.5581 - val_loss: 8.2342 - val_acc: 0.2889\n",
      "2515/2515 [==============================] - 0s 60us/step\n",
      "Test Loss and Accuracy -> [8.004922602333084, 0.30417495998544675]\n",
      "result predicted: [[6.38300124e-09 9.14189779e-10 5.97722483e-10 1.18052237e-01\n",
      "  4.66755091e-06 1.07302936e-02 1.47076847e-04 3.96895084e-05\n",
      "  4.86434804e-09 2.37704087e-02 3.17670554e-02 1.30378919e-09\n",
      "  1.26626603e-07 1.66315986e-05 1.01394444e-05 4.99684866e-05\n",
      "  4.11606015e-06 1.18115490e-10 2.54634314e-10 1.25956817e-07\n",
      "  4.95398944e-10 1.02976344e-07 2.14722569e-08 1.58677325e-02\n",
      "  3.78591352e-07 1.05327111e-07 2.79249207e-07 1.40349934e-04\n",
      "  3.45125341e-06 1.12182803e-07 7.68025421e-10 3.67199732e-06\n",
      "  3.36335052e-06 1.56834381e-06 3.82555010e-09 1.54921267e-07\n",
      "  3.10066594e-06 1.48809960e-07 1.62201650e-05 8.17669932e-09\n",
      "  8.03864350e-06 2.79619653e-05 2.73017111e-08 5.78379428e-11\n",
      "  3.23943533e-02 1.14946021e-02 7.03376088e-07 1.22388292e-06\n",
      "  3.84820922e-08 1.62555662e-03 2.45344290e-05 1.53758762e-07\n",
      "  2.51394295e-06 6.37231850e-08 5.66807823e-10 6.64965398e-08\n",
      "  3.47106377e-14 2.07100967e-08 8.38086853e-05 4.01692084e-08\n",
      "  3.72159411e-06 2.20340595e-01 7.27319775e-06 2.29612634e-01\n",
      "  1.87943969e-03 8.24380293e-03 9.75292451e-08 5.06018449e-10\n",
      "  5.08345508e-08 2.32939534e-02 1.84599558e-09 5.42728119e-11\n",
      "  1.12084075e-13 1.56196208e-07 7.08508641e-02 2.46480401e-08\n",
      "  1.90843254e-01 1.23360969e-05 1.20687446e-07 4.45229809e-09\n",
      "  8.33252856e-10 1.61018994e-04 2.32627020e-08 5.14536396e-08\n",
      "  3.00765670e-08 4.77271713e-03 9.31119182e-09 3.93946264e-10\n",
      "  3.44606851e-05 2.17363709e-06 1.45410217e-04 4.88415571e-05\n",
      "  6.33938089e-06 4.57436578e-08 4.72231754e-10 2.15297700e-14\n",
      "  8.05579930e-07 1.07425507e-07 3.69542583e-08 8.84998656e-07\n",
      "  7.25415816e-07 7.35225270e-10 4.70382038e-05 4.47279186e-10\n",
      "  2.91454660e-09 2.25985855e-06 1.41357384e-06 3.16788937e-05\n",
      "  1.17147109e-03 4.57176412e-07 2.28587351e-06 4.51028659e-11\n",
      "  7.14815851e-09 1.94772128e-05 1.21955409e-05 1.79944004e-09\n",
      "  1.15335354e-06 2.32663844e-08 7.14905254e-05 1.23530149e-03\n",
      "  8.27476470e-05 9.68833831e-08 5.67908387e-10 2.42423831e-10\n",
      "  8.39465120e-09 2.41859729e-04 1.05551453e-05 3.87944747e-05\n",
      "  1.01014210e-08 2.41518222e-07 8.90190464e-08 1.79987353e-06\n",
      "  2.24904725e-05 9.03021137e-05 8.29939963e-05 5.71668425e-05\n",
      "  5.74941225e-07 2.11117003e-06 2.33748718e-08 5.08808662e-10\n",
      "  4.06066398e-08 4.06248291e-05 1.41695281e-07 3.15238822e-06\n",
      "  4.59746207e-06 3.23568258e-07 1.49646269e-06 6.56195407e-05\n",
      "  6.05643936e-06 5.00577735e-05 5.26705116e-06 2.98902052e-07\n",
      "  8.71575025e-08 2.89796444e-05 4.21339610e-06 1.69213627e-06\n",
      "  1.53587877e-07 2.67799938e-09 1.54938250e-11]]\n",
      "result real: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "result top 10: [63, 61, 76, 3, 74, 44, 10, 9, 69, 23]\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# dataset\n",
    "dataset = \"bibtex\"\n",
    "data_dir = 'data/{}'.format(dataset)\n",
    "\n",
    "def load_input():\n",
    "    data = list(loadmat(data_dir + '/input.mat')['data'][0][0])\n",
    "    return data[:4]\n",
    "\n",
    "\n",
    "class Deep:\n",
    "    def __init__(self, batch_size, epochs, learning_rate, input_dim, output_dim):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.model = self.classifier()\n",
    "\n",
    "    def classifier(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, activation='relu', input_dim=self.input_dim))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(self.output_dim, activation='softmax'))\n",
    "\n",
    "        optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.model.fit(x, y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2)\n",
    "\n",
    "    def print_performance(self, x, y):\n",
    "        performance_test = self.model.evaluate(x, y, batch_size=self.batch_size)\n",
    "        print('Test Loss and Accuracy ->', performance_test)\n",
    "\n",
    "\n",
    "class Wide:\n",
    "    def __init__(self, batch_size, epochs, learning_rate, input_dim, output_dim):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.model = self.classifier()\n",
    "\n",
    "    def classifier(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.output_dim, activation='softmax', input_dim=self.input_dim))\n",
    "\n",
    "        optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.model.fit(x, y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2)\n",
    "\n",
    "    def print_performance(self, x, y):\n",
    "        performance_test = self.model.evaluate(x, y, batch_size=self.batch_size)\n",
    "        print('Test Loss and Accuracy ->', performance_test)\n",
    "\n",
    "\n",
    "class WideAndDeep:\n",
    "    def __init__(self, batch_size, epochs, learning_rate, input_dim, output_dim):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.model = self.classifier()\n",
    "\n",
    "    def classifier(self):\n",
    "        optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "        # wide part\n",
    "        wide = Input(shape=(self.input_dim,))\n",
    "\n",
    "        # deep part\n",
    "        deep_input = Input(shape=(self.input_dim,))\n",
    "        deep = Dense(100, activation='relu')(deep_input)\n",
    "        deep = Dense(50, activation='relu')(deep)\n",
    "\n",
    "        # concatenate : wide and deep\n",
    "        wide_n_deep = concatenate([wide, deep])\n",
    "        wide_n_deep = Dense(self.output_dim, activation='softmax')(wide_n_deep)\n",
    "        model = Model(inputs=[wide, deep_input], outputs=wide_n_deep)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, wide_x, deep_x, y):\n",
    "        self.model.fit([wide_x, deep_x], y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.2)\n",
    "\n",
    "    def print_performance(self, wide_x, deep_x, y):\n",
    "        performance_test = self.model.evaluate([wide_x, deep_x], y, batch_size=self.batch_size)\n",
    "        print('Test Loss and Accuracy ->', performance_test)\n",
    "\n",
    "\n",
    "def main(model_param):\n",
    "    # prepare dataset\n",
    "    x_train, y_train, x_test, y_test = load_input()\n",
    "    x_train = x_train.toarray()\n",
    "    y_train = y_train.toarray()\n",
    "    x_test = x_test.toarray()\n",
    "    y_test = y_test.toarray()\n",
    "\n",
    "    # prepare hyper parameter\n",
    "    batch_size = 500\n",
    "    epochs = 30\n",
    "    learning_rate = 0.001\n",
    "    input_dim = x_train.shape[1]\n",
    "    output_dim = y_train.shape[1]\n",
    "    \n",
    "\n",
    "    if model_param == \"deep\":\n",
    "        deep = Deep(batch_size, epochs, learning_rate, input_dim, output_dim)\n",
    "        deep.fit(x_train, y_train)\n",
    "        deep.print_performance(x_test, y_test)\n",
    "    elif model_param == 'wide':\n",
    "        wide = Wide(batch_size, epochs, learning_rate, input_dim, output_dim)\n",
    "        wide.fit(x_train, y_train)\n",
    "        wide.print_performance(x_test, y_test)\n",
    "    else:\n",
    "        wide_n_deep = WideAndDeep(batch_size, epochs, learning_rate, input_dim, output_dim)\n",
    "        wide_n_deep.fit(x_train, x_train, y_train)\n",
    "        wide_n_deep.print_performance(x_test, x_test, y_test)\n",
    "\n",
    "        # prediction for individual and y_column rank\n",
    "        x_predict_test = x_test[np.newaxis, 0, :]\n",
    "        y_predict_test = y_test[0]\n",
    "        result = wide_n_deep.model.predict([x_predict_test, x_predict_test])\n",
    "        print('result predicted:', result)\n",
    "        print('result real:', y_predict_test)\n",
    "\n",
    "        # select top 10 y's column index in result(softmax prediction)\n",
    "        top_10_y_column = result[0].argsort()[-10:][::-1].tolist()\n",
    "        print('result top 10:', top_10_y_column)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main('widendeep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
